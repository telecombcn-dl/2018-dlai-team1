---
title: "Reproduced Paper"
bg: blue
color: white
fa-icon: quote-left
---
<p style='text-align: justify;'>

*Despite recent progress, Generative Adversarial Networks (GANs) still suffer from training instability, requiring careful consideration of architecture design choices and hyper-parameter tuning. The reason for this fragile training behaviour is partially due to the discriminator performing well very quickly; its loss converges to zero, providing no reliable backpropagation signal to the generator. In this work we introduce a new technique - progressive augmentation of GANs (PAGAN) - that helps to overcome this fundamental limitation and improve the overall stability of GAN training. The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input space, thus enabling continuous learning of the generator. We show that the proposed progressive augmentation preserves the original GAN objective, does not bias the optimality of the discriminator and encourages the healthy competition between the generator and discriminator, leading to a better-performing generator. We experimentally demonstrate the effectiveness of the proposed approach on multiple benchmarks (MNIST, Fashion-MNIST, CIFAR10, CELEBA) for the image generation task.*
</p>

<!-- If you find this work useful, please consider citing:

<i>
Daniel Fojo, Victor Campos, Xavier Giro-i-Nieto. "Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks", In International Conference on Learning Representations Workshop Track, 2018.
</i>

<pre>
@inproceedings{fojo2018repeat,
title={Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks},
author={Fojo, Daniel, and Campos, V{\'\i}ctor and Giro-i-Nieto, Xavier},
booktitle={International Conference on Learning Representations Workshop Track},
year={2018}
}
</pre> -->

You can find the paper in [openreview.net](https://openreview.net/pdf?id=ByeNFoRcK7), or download the PDF from [here](https://openreview.net/pdf?id=ByeNFoRcK7).
