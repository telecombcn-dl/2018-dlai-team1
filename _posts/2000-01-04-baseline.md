---
title: "Approach"
bg: blue
color: white
style: center
fa-icon: cog
---

<p style='text-align: justify;'>

To begin with, each of us read the paper and did a first meeting in order to share all the knowledge and doubts originated. The architecture used was pretty basic; the only tough point was to understand the progressive augmentation (PA) technique, which is the key of the whole paper.
<br />
<br />
Once having solved all our doubts, we proceed in starting to code. Here, we divided the work in two parts: some of us had to search for already-implemented code of the baseline architecture used (we couldnâ€™t find any published code directly related to the paper) and the others started working on coding those parts related to the progressive augmentation.
<br />
<br />
Although in the paper two main networks were used (Spectral Normalization DCGAN and InfoGAN), we decided to limit our study in just the fist network. Regarding this first newtork, we found it implemented in tensorflow <a href="https://github.com/minhnhat93/tf-SNDCGAN">here</a>.
<br />
<br />
We were also working on finding already implemented code for the KID metric when we realised that we were experimenting too many problems when integrating self-implemented code (basically for the PA part)  with the baseline, mainly due to the static of tensorflow.
Hence, we decided to change our strategy and implement everything from scratch in pytorch.  It turned out to be the best solution, as there is a DCGAN tutorial in <a href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html">pytorch wepage</a> and Spectral Normalization is already implemented in the latest version.
<br />
<br />
Once we had all the code ready, we debug it separately (in order to ensure there was no error in the code), and after checking all the hyperparameters were the same as the ones specified in the paper, we start to run all the experiments.
</p>
